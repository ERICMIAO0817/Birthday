<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Bert学习记录 | Gimdong</title><meta name="keywords" content="NLP"><meta name="author" content="Eric Miao,1838040569@qq.com"><meta name="copyright" content="Eric Miao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="参与NLP有关项目，记录学习所得。">
<meta property="og:type" content="article">
<meta property="og:title" content="Bert学习记录">
<meta property="og:url" content="http://example.com/2022/05/01/BERT/index.html">
<meta property="og:site_name" content="Gimdong">
<meta property="og:description" content="参与NLP有关项目，记录学习所得。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://4myblog.oss-cn-beijing.aliyuncs.com/img/d1st99u-0493e064-ee96-4686-a7ef-e905aeecc12f.png">
<meta property="article:published_time" content="2022-05-01T13:39:02.000Z">
<meta property="article:modified_time" content="2022-07-17T18:11:53.877Z">
<meta property="article:author" content="Eric Miao">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://4myblog.oss-cn-beijing.aliyuncs.com/img/d1st99u-0493e064-ee96-4686-a7ef-e905aeecc12f.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2022/05/01/BERT/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?hm.src = "https://hm.baidu.com/hm.js?c38d2c39cb578ef8cd93f733212f453e";";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Bert学习记录',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-07-18 02:11:53'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="Gimdong" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 类别</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('http://4myblog.oss-cn-beijing.aliyuncs.com/img/children-comedy-family-muppets-wallpaper-preview.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Gimdong</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 类别</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Bert学习记录</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-05-01T13:39:02.000Z" title="发表于 2022-05-01 21:39:02">2022-05-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-07-17T18:11:53.877Z" title="更新于 2022-07-18 02:11:53">2022-07-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0/">技术学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Bert学习记录"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><em>写在前面：本人刚刚入门NLP，希望通过记录博客来巩固自己的知识，增进对知识的理解。</em></p>
<p>在之前的博客，我们进行了CRF的原理探寻以及借助CRF工具包实现各类序列标注任务，如中文分词、NER、拼音输入法等等。现在，让我们再上一个台阶，从统计自然语言模型到神经网络自然语言模型。由于最近在进行阅读理解（machine reading comprehension）的学习，因此选择bert这一微调模型的经典之作进行学习记录。现有的Bert可参考的博文也很多，我以个人的视角进行了精华提取，希望能对读者有所帮助。</p>
<p>Bert论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p>
<h3 id="Bert是什么？"><a href="#Bert是什么？" class="headerlink" title="Bert是什么？"></a>Bert是什么？</h3><p>Bert，全称为：<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers，即双向性Transformer编码器。从它的名字我们可以得知，Bert的要点是：<strong>双向性</strong>+<strong>Transformer Encoder</strong>。接下来，我会围绕这两个要点，分别谈谈我自己的学习心得与看法，仅供参考，希望对你有帮助。</p>
<h3 id="Bert解决了什么问题？"><a href="#Bert解决了什么问题？" class="headerlink" title="Bert解决了什么问题？"></a>Bert解决了什么问题？</h3><p><strong>先说结论：Bert为NLP任务提供了泛化性强、效果显著的预训练模型。</strong></p>
<h4 id="什么是预训练？为什么这么重要？"><a href="#什么是预训练？为什么这么重要？" class="headerlink" title="什么是预训练？为什么这么重要？"></a>什么是预训练？为什么这么重要？</h4><p>在CV（图像）领域，有许多预训练模型和对应的预训练权重文件提供给公众使用。这些模型往往是在很大的数据集上（如ImageNet）已经进行了很彻底的训练，我们需要的时候直接对模型进行微调即可。</p>
<p>预训练与微调的关系就好比说，我现在有一个神经网络，它有50层深。<strong>开始</strong>的时候，我给它的数据集是各种品牌汽车的图片，里面有保时捷、宝马等等并且我也做好了数据集的标注，希望训练出一个能根据车辆图片<strong>识别出汽车品牌</strong>的神经网络。</p>
<p>训练完成，验证集上也获得了不错的效果后，我被告知：不需要一个能识别品牌的模型，只需要一个能<strong>识别出车型</strong>的模型，比如轿车、SUV、房车等等，但是这个任务的数据集又<strong>很小</strong>。那该怎么办呢？推翻重来？重新训练？其实不需要。你可以把你开始时训练的模型当作<strong>预训练</strong>模型，在上面根据你新的数据集进行<strong>微调</strong>。这样为什么有效呢？</p>
<p>答案是，神经网络模型的特点决定的这一切有效。在残差引入卷积网络之后，经典的卷积网络都走向<strong>窄而深</strong>的发展方向。在较浅的隐藏层，网络会学到初级的一些特征，比如车的轮廓、大体形状。再深一些的隐藏层，网络会学到更接近任务需求的特征，比如车的流形、车头的长相。对于我们目前遇到的新任务，其实浅层的网络参数不需要再重新学习了，因为车的轮廓和形状对我们很有用，我们直接<strong>冻结</strong>住这些参数。但是高层一些的特征<strong>或许不那么重要</strong>，我们可以对高层网络参数进行<strong>微调</strong>，比如直接重新训练softmax层，或者是重新训练没被冻结的隐藏层等等。</p>
<p>到这里，你应该明白了为什么预训练模型重要：因为实际生活中的任务很多样，为每个任务重新训练模型成本很高，也不见得有好的效果。</p>
<h4 id="那NLP为什么到Bert之前，都没有这样的一种体系？"><a href="#那NLP为什么到Bert之前，都没有这样的一种体系？" class="headerlink" title="那NLP为什么到Bert之前，都没有这样的一种体系？"></a>那NLP为什么到Bert之前，都没有这样的一种体系？</h4><p>个人认为，这是因为在NLP领域，Bert出现之前，还尚未有很明确的知识告诉人们，越深的神经网络对自然语言处理也同样越有效，而且NLP的任务比CV复杂许多，图像说到底就是像素点，但是语言任务有处理词的、处理句子的、处理文章的，<strong>最小单位都不大相同</strong>，不同语种之间也有许多的<strong>语言性差异</strong>。但是预训练其实在NLP领域意义重大，因为许多语料数据要进行收集的话，可以很轻松地获得<strong>大体量的无标签数据集</strong>，但是要为各个任务打上标签，那将是很庞大乃至难以想象的工作量。一个好的预训练模型，可以大大提高NLP模型的落地应用转化率。</p>
<p>其实在Bert之前，也有许多工作在朝着这个方向努力。大体来说，主要是两种策略：</p>
<p>基于<strong>特征</strong>的预训练（feature-based）VS 基于<strong>微调</strong>的预训练（fine-tune）</p>
<p>前者的代表作是：<strong>ELMo</strong></p>
<p>后者的代表作是：<strong>OpenAPI GPT</strong></p>
<p>两者可以分别理解为：</p>
<p><strong>feature-based：</strong>基于<strong>网络</strong>的调节，针对预训练得到的输出，还要设计相应的网络来应对不同的task。</p>
<p><strong>fine-tune：</strong>基于<strong>参数</strong>的调节，针对预训练得到的网络进行网络参数的微调。</p>
<p>而Bert很明显，应该是属于后者这种接近CV的预训练策略。</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220502200543849.png" alt="image-20220502200543849"></p>
<p>上图是Bert原文中，与GPT和ELMo做的对比。GPT和ELMo的内容不是本文重点，所以就根据上图进行简要的解释吧。</p>
<p>ELMo采用的是两个反向的LSTM网络进行训练，试图让两个网络的知识涵盖上下文信息。但实际上，这样直接的将一个从左到右的网络和从右到左的网络进行叠加，并不能在每一层都有效整合上下文信息。对于ELMo而言，主要需求是<strong>获取更多的语言特征</strong>，因此，ELMo的输出其实就是一个Word Embedding，对每个词进行了<strong>特征维度</strong>的扩展。</p>
<p>GPT采用的是transformer的<strong>解码器</strong>，是一个从左到右的模型。其实Bert和GPT的架构是类似的，都是transformer为基础，只不过Bert采用的是<strong>编码器</strong>，引入了双向性。GPT模型中，每一个词只能根据之前的词是什么来预测下一个词，不能结合下文信息进行预测。</p>
<p>在Bert之前的预训练模型与策略都有一些绕不开的局限性：<strong>上下文信息难以有效整合、句子层面的任务难以与字词层面的任务在一个预训练模型上相适应。</strong></p>
<p>而Bert是集大成者，即保留了<strong>微调</strong>的思路，又引入了<strong>上下文信息</strong>，还兼顾了<strong>token</strong>-level与<strong>sentence</strong>-level的任务。</p>
<p>但是这种双向设计的transformer编码器，其实给Bert<strong>上了一把锁</strong>，具体是什么呢？我们继续往下看。</p>
<h3 id="双向性（Bidirectional）的体现"><a href="#双向性（Bidirectional）的体现" class="headerlink" title="双向性（Bidirectional）的体现"></a>双向性（Bidirectional）的体现</h3><p>在Bert中，双向性主要由<strong>掩膜语言模型、句子语序预测、自我注意力机制</strong>体现的。</p>
<h4 id="掩膜语言模型–MLM"><a href="#掩膜语言模型–MLM" class="headerlink" title="掩膜语言模型–MLM"></a>掩膜语言模型–MLM</h4><p>全称，masked language model。其实说白了，就是对输入的句子里面的token进行掩盖（加[mask]），然后让模型<strong>预测</strong>mask掉的词是什么。文中举的例子是这样的：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220502231721222.png" alt="image-20220502231721222"></p>
<p>值得注意的是，并不是所有的token都会被mask掉，实际上是取输入中15%的token选中进行mask，并且被选中的token也只有80%的几率会被mask，还有10%是替换成别的词，以及10%的概率不mask。</p>
<p>这样设计的意义是什么呢？</p>
<p>个人认为，是通过加入了约束规则迫使模型主动地去<strong>学习上下文知识</strong>。如果不给予模型一个任务，很难控制模型的收敛走向。掩膜预测的任务能够帮助模型注重上下文信息，结合这些信息来推断某个token的意思。从这个角度也使得这个token的特征维度得到拓展，不止是token本身，还有上下文中与它相关的知识。</p>
<h4 id="句子语序预测–NSP"><a href="#句子语序预测–NSP" class="headerlink" title="句子语序预测–NSP"></a>句子语序预测–NSP</h4><p>全称，next sentence prediction。这个任务主要是对输入的句子对是否是顺承关系进行预测，句子对AB的中间以及结尾以[SEP]进行分隔，让模型判断B句子是否是A句子的下一句话。文中举的例子是这样的：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503010256142.png" alt="image-20220503010256142"></p>
<p>这个任务看起来很简单，也很好理解。后文的消融实验其实证明了它的作用并不显著，但是我认为NSP任务的设计，是为了将模型能更好地从token-level迁移到sentence-level。同时，我也认为MLM和NSP的设计都是为了弥补transformer本身<strong>缺乏序列信息</strong>的特点。</p>
<p><em>一点补充说明：RNN在<strong>序列信息</strong>的学习其实比起transformer要更加彻底一些，因为transformer会将序列里的每一个token都做自我注意力，导致你的token以任意排列顺序输入都不会有很大影响。所以transformer原本的论文加入了<strong>位置编码</strong>来缓解这一缺陷。</em></p>
<p>自我注意力机制会放在encoder的部分继续讲述。</p>
<h3 id="Transformer-Encoder"><a href="#Transformer-Encoder" class="headerlink" title="Transformer Encoder"></a>Transformer Encoder</h3><p>Bert的另一大要点，就是基于transformer的编码器作为网络架构。Bert的基本模型采用了12层编码器堆叠的架构：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503012627364.png" alt="image-20220503012627364"></p>
<p>上图灰色的矩形内是编码器的基本架构，Bert基本上没有改变transformer的原本设计，直接引用了这个模块。关于编码器，我想，最重要的部分就是：<strong>Multi-Head Attention</strong>。</p>
<h4 id="什么是Attention？"><a href="#什么是Attention？" class="headerlink" title="什么是Attention？"></a>什么是Attention？</h4><p>顾名思义，是<strong>注意力</strong>。你可以将两个向量之间的距离理解为注意力，离得越近，说明我越注意你；离得越远，说明我不需要怎么关心你。讲到向量，讲到距离，很自然地会联想到点乘，因为<strong>余弦</strong>可以在夹角层面上反应向量之间的距离，或者说，差异性。所以，transformer的原作者采用的就是这种思路来实现注意力：<strong>点乘注意力机制（Dot-Product Attention）</strong>。</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503014818044.png" alt="image-20220503014818044"></p>
<p>上图是点乘注意力机制的<strong>计算图</strong>。可以看到，基本思路是，将三个输入Q、K、V读入，其中Q与K进行矩阵相乘，进行尺度缩放之后，不掩膜的话就直接softmax得到注意力分数，再将这个分数与V相乘，得到最终结果。数学一点的表示是这样：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503015328447.png" alt="image-20220503015328447"></p>
<p>是不是<strong>很懵</strong>？没关系，我们一步步来。</p>
<p>首先让我们明确，什么是Q、K、V。<strong>Q是Query，意为查询；K是Key，意为键；V是Value，意为值。</strong>从编码器的结构也可以看出，Q、K、V的输入其实都是<strong>同一个东西</strong>，比如就是上一个隐藏层的输出。在上面的计算图中，我们实际上是通过Q和K的相乘来获得各个token之间的注意力。在这个过程中，矩阵Q的每一行和转置后的矩阵K的每一列都能做向量相乘，相当于是每一个token都和包括自己的其它token进行了计算。因此，可以将Q视作”查询“，代表我现在<strong>计算到了哪一个token</strong>；而K视作”键“，代表我现在<strong>针对我查询的token进行相对应的各个键的注意力计算</strong>。而V又是什么呢？实际上，Q与K相乘得到的结果，就代表了这段序列内部各个token与每个token之间的<strong>关系信息</strong>，乘以V实际上是将这种关系信息以权重的形式传给原本的输入，让它知道它本身的注意力信息是什么，自己内部的哪些部分联系更紧密、哪些部分关系不大。</p>
<p>那么，除以${\sqrt{d_{k}}}$又是什么意思呢？这里就是计算图上标题<strong>”scaled“</strong>的体现。原本我认为，除以这个数字单纯是防止对角线上的值过大（因为对角线是某个token和自己相乘，结果是1），把尺度缩小来减轻影响，但是经过学长点拨之后：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503020611362.png" alt="image-20220503020611362"></p>
<p>发现还有这样更加数学的解释，amazing！</p>
<p>初步理解attention之后，让我们继续看看什么是”multi-head“。</p>
<h4 id="什么是”Multi-Head“？"><a href="#什么是”Multi-Head“？" class="headerlink" title="什么是”Multi-Head“？"></a>什么是”Multi-Head“？</h4><p>多头，顾名思义，是在注意力机制的基础之上，多加了好多个”头“。可以简单理解为将上面的计算过程提前分成了好几份分开计算：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503021020604.png" alt="image-20220503021020604"></p>
<p>上图是多头注意力的模式图解，中间紫色框框内部就是我们刚刚讲到的点乘注意力模块。假设头的数目是h，那么其实就是将Q、K、V分成h份，各自进行点乘注意力。总共就是进行了h次。而且在输入处还能看到，每个头的Q、K、V都乘了一个矩阵（linear）进行映射。数学一点的表示是这样：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503103128972.png" alt="image-20220503103128972"></p>
<p>可以看到，多头做的事情其实就是将各个attention的结果拼接一下，再乘以一个输出矩阵融合信息。这里值得一提的是：为什么要乘以矩阵呢？其实，主要是因为Bert的训练资料比较丰富，可以拥有更多的可学习空间。如果不乘这个矩阵的话，其实分为多个头和不分多头直接点乘attention的结果没有什么区别。引入这几个矩阵之后，能提供更多的<strong>变化空间</strong>，让模型尽可能学到attention的<strong>多种模式</strong>。</p>
<p><em>在transformer原作的论文中也提到，这样的方式其实不会对计算量有更大要求，和一次计算完没什么区别。</em></p>
<p>了解完注意力机制后，让我们来看看Bert的整体<strong>结构</strong>。</p>
<h3 id="Bert基本结构–预训练"><a href="#Bert基本结构–预训练" class="headerlink" title="Bert基本结构–预训练"></a>Bert基本结构–预训练</h3><h4 id="Bert的三层嵌入"><a href="#Bert的三层嵌入" class="headerlink" title="Bert的三层嵌入"></a>Bert的三层嵌入</h4><p>Bert的结构中，对于输入的token进行了三层嵌入（embedding）：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503114726473.png" alt="image-20220503114726473"></p>
<ol>
<li><p>首先是<strong>Token Embedding</strong>：</p>
<p>这个步骤其实很简单，只是把输入的token乘以一个嵌入矩阵提升维度，为后续嵌入做准备。</p>
<p><strong>值得一提的是：</strong>输入的token其实是经过了WordPiece的词根词缀字典查找获得的，所以会看到诸如”##ing“这样的形式，表示它不是一个完整的单词。WordPiece的字典大概是3w字量级。</p>
<p>还有一点是输入的开头，有个[CLS]token，表示输入的开始。每一层编码器的开头都含有这个[CLS]，可以理解为整个block的一个代表，包括最后做分类任务的话，也是以[CLS]作为整个模型的信息融合结果的代表，进行分类。</p>
</li>
<li><p>接下来，是<strong>Segment Embedding</strong>：</p>
<p>这个嵌入部分是与NSP配合使用的，由于需要判断句子对是否有顺承关系，就要先对它们事先进行标记。以”A“代表第一句话，”B“代表第二句话。所以它的嵌入维度是：<strong>2*768</strong>。（图中小细节：第一个[SEP]属于A，第二个[SEP]属于B。）</p>
</li>
<li><p>最后，是<strong>Position Embedding</strong>：</p>
<p>Bert当中的位置编码与transformer中的实现<strong>不同</strong>，transformer原本工作中的位置编码是通过公式计算得到的：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503151315841.png" alt="image-20220503151315841"></p>
<p>而Bert不同，Bert可学习参数足够多，也需要足够的空间来充分学习语义特征，因此Bert当中的位置编码也是一个<strong>可学习的嵌入</strong>。我们事先给好各个token对应的位置id（不大于512），然后初始化一个<strong>512*768</strong>大小的嵌入。</p>
</li>
</ol>
<p>或许你会奇怪，为什么经常看到<strong>512、768、一对输入</strong>？</p>
<p>这是由于Bert预训练的初始设置决定的。一开始google预训练的时候，就<strong>设置了输入是一句或者一对句子，最长长度不超过512，隐藏层大小是768。</strong></p>
<h4 id="直观一些，看看源码"><a href="#直观一些，看看源码" class="headerlink" title="直观一些，看看源码"></a>直观一些，看看源码</h4><p><em>这里使用的是huggingface的pytorch版本Bert，比起TF版本感觉更好看明白一些。详细的讲解可以参考文章：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/369012642">https://zhuanlan.zhihu.com/p/369012642</a></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BertEmbeddings</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Construct the embeddings from word, position and token_type embeddings.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)</span><br><span class="line">        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span></span><br><span class="line">        <span class="comment"># any TensorFlow checkpoint file</span></span><br><span class="line">        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line">        <span class="comment"># position_ids (1, len position emb) is contiguous in memory and exported when serialized</span></span><br><span class="line">        self.position_embedding_type = <span class="built_in">getattr</span>(config, <span class="string">&quot;position_embedding_type&quot;</span>, <span class="string">&quot;absolute&quot;</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&quot;position_ids&quot;</span>, torch.arange(config.max_position_embeddings).expand((<span class="number">1</span>, -<span class="number">1</span>)))</span><br><span class="line">        <span class="keyword">if</span> version.parse(torch.__version__) &gt; version.parse(<span class="string">&quot;1.6.0&quot;</span>):</span><br><span class="line">            self.register_buffer(</span><br><span class="line">                <span class="string">&quot;token_type_ids&quot;</span>,</span><br><span class="line">                torch.zeros(self.position_ids.size(), dtype=torch.long),</span><br><span class="line">                persistent=<span class="literal">False</span>,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, input_ids=<span class="literal">None</span>, token_type_ids=<span class="literal">None</span>, position_ids=<span class="literal">None</span>, inputs_embeds=<span class="literal">None</span>, past_key_values_length=<span class="number">0</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="keyword">if</span> input_ids <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            input_shape = input_ids.size()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input_shape = inputs_embeds.size()[:-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> position_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs</span></span><br><span class="line">        <span class="comment"># when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves</span></span><br><span class="line">        <span class="comment"># issue #5664</span></span><br><span class="line">        <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">hasattr</span>(self, <span class="string">&quot;token_type_ids&quot;</span>):</span><br><span class="line">                buffered_token_type_ids = self.token_type_ids[:, :seq_length]</span><br><span class="line">                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[<span class="number">0</span>], seq_length)</span><br><span class="line">                token_type_ids = buffered_token_type_ids_expanded</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> inputs_embeds <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            inputs_embeds = self.word_embeddings(input_ids)</span><br><span class="line">        token_type_embeddings = self.token_type_embeddings(token_type_ids)</span><br><span class="line"></span><br><span class="line">        embeddings = inputs_embeds + token_type_embeddings</span><br><span class="line">        <span class="keyword">if</span> self.position_embedding_type == <span class="string">&quot;absolute&quot;</span>:</span><br><span class="line">            position_embeddings = self.position_embeddings(position_ids)</span><br><span class="line">            embeddings += position_embeddings</span><br><span class="line">        embeddings = self.LayerNorm(embeddings)</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure>

<p>基本上可以根据代码设计来对照Bert论文给的图片一步步推导。</p>
<p><strong>值得注意的是：</strong>计算完三种嵌入并将他们相加之后，需要进行<strong>LayerNorm+Dropout</strong>。LayerNorm是transformer原本工作就使用的归一化trick，与BatchNorm不同，LN的方式是在<strong>单个样本</strong>的维度上做归一化，而BN是在整个batch中做<strong>全局</strong>归一化。LN对于NLP任务来说更加合理，因为输入的长短不一，BN的话会出现很多向量长度不同，要补零，影响全局归一化。</p>
<h3 id="Bert如何微调"><a href="#Bert如何微调" class="headerlink" title="Bert如何微调"></a>Bert如何微调</h3><p>讲完了Bert预训练的几大要点，让我们来看看Bert是怎么进行微调以适应更多task的。</p>
<h4 id="自然语言推理–NLI"><a href="#自然语言推理–NLI" class="headerlink" title="自然语言推理–NLI"></a>自然语言推理–NLI</h4><p>自然语言推理任务简单来说，就是根据句子对，来推理它们之间的关系，可以视作句子对的分类问题。Bert论文中给出的示意图如下：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503200148095.png" alt="image-20220503200148095"></p>
<p>其实NLI任务本身就很贴合Bert的设计，与NSP任务也很接近。[CLS]这个class token也正好可以作为分类任务的输出。微调时只需要在[CLS]的输出上面加上一层或几层线性分类器，训练分类器即可。</p>
<h4 id="单句分类任务–文本分类、情感分析"><a href="#单句分类任务–文本分类、情感分析" class="headerlink" title="单句分类任务–文本分类、情感分析"></a>单句分类任务–文本分类、情感分析</h4><p>与NLI不同，这里的情况是输入不分为上下两部分，但任务依旧是分类任务。示意图：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503201408926.png" alt="image-20220503201408926"></p>
<p>思路与NLI相同，也是在[CLS]上加线性分类器。</p>
<h4 id="阅读理解任务–MRC"><a href="#阅读理解任务–MRC" class="headerlink" title="阅读理解任务–MRC"></a>阅读理解任务–MRC</h4><p>阅读理解，machine reading comprehension。阅读理解的任务广度很大，这里主要以QA举例子。也就是我输入一个问题加一篇文章，你要在文章中找到一个部分作为答案。示意图：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503203037531.png" alt="image-20220503203037531"></p>
<p>那我们是如何利用Bert做QA的呢？其实答案很粗暴，就是文章中对于每个token，分别预测它们作为答案开头和结尾的概率有多高。所以，微调的时候，会对每一个token分别学习两个向量：一个判断它是否作为开头token，一个判断它是否作为结尾token。再加上softmax获得各个token作为开头或者结尾的可能性，取开头中最大概率的和结尾中最大概率的，并将中间内容输出。</p>
<p><strong>注意</strong>，这里就已经是token-level了。因为你可以看到，我们是对最后一层的所有属于文章的token进行处理，不再只是拿[CLS]作为代表。</p>
<h4 id="序列标注问题–NER"><a href="#序列标注问题–NER" class="headerlink" title="序列标注问题–NER"></a>序列标注问题–NER</h4><p>序列标注问题就是很典型的token-level的问题，判断每一个token的标签。示意图：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503204132483.png" alt="image-20220503204132483"></p>
<p>这里其实也很好理解，和QA一样是对每个token的输出做处理。但是不一样的地方在于，QA需要两次计算，算作为开头和结尾的概率。但是NER的话，只用在每个token上加一个类别的分类器来微调即可。</p>
<p><strong>值得注意的是</strong>：其实这种NER方法依然限制在序列标注本身领域之内，只能对一串句子获得一组标签。但实际上我们知道，像：</p>
<p><strong>”中国传媒大学“</strong>里面，”中国传媒大学“可以视作<strong>大学</strong>这个命名实体，但是”中国“也是<strong>国家</strong>层面的命名实体。这种交叠的（<strong>nested</strong>）命名实体问题不能用传统思路解决。<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.11476v6.pdf">香浓科技的这篇论文</a>提供了一个新思路：<strong>用MRC对NER问题重新建模</strong>，取得了不错的效果。这也会是我接下来的学习方向，后续会更进这篇文章与我自己的想法。</p>
<h3 id="所以，Bert到底学到了什么"><a href="#所以，Bert到底学到了什么" class="headerlink" title="所以，Bert到底学到了什么"></a>所以，Bert到底学到了什么</h3><p>看到这里，希望你对Bert是什么已经有了一定的了解。那么，让我们回到梦开始的地方，<strong>预训练</strong>。</p>
<p>上面说，CV中预训练可行的原因是，神经网络窄而深，并且不同的层级有学习到由浅到深不同的特征，这使得微调效果卓著。</p>
<p>那么Bert做到了吗？Bert各个层是否也学习了<strong>由浅到深</strong>不同层级的语义信息呢？</p>
<p>怀着这个问题，我看到了这篇文章：<a target="_blank" rel="noopener" href="https://hal.inria.fr/hal-02131630/document">What does BERT learn about the structure of language?</a>文章用很多分析手段从多角度研究了这个问题，这里我简要的记录一下。</p>
<h4 id="短句句法特征"><a href="#短句句法特征" class="headerlink" title="短句句法特征"></a>短句句法特征</h4><p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503205437187.png" alt="image-20220503205437187"></p>
<p>这张图片，原文的意思是大概是说，他们采用了对LSTM相同的研究手段来研究Bert对于短语级别的结构信息的捕捉情况。可以看出，Bert的前两层色块之间有明显的区分，说明Bert能捕捉到<strong>短语级别</strong>的特征信息，但是这些信息在高层（最后两层）消失了，说明低级特征确实没有表现在高层之中。</p>
<h4 id="三级任务分析"><a href="#三级任务分析" class="headerlink" title="三级任务分析"></a>三级任务分析</h4><p>在这一模块，作者研究了Bert在三大方面信息获取的表现：</p>
<ol>
<li><p>表层信息–Surface</p>
</li>
<li><p>句法信息–Syntactic</p>
</li>
<li><p>语义信息–Semantic</p>
</li>
</ol>
<p>结果如下：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503210403289.png" alt="image-20220503210403289"></p>
<p>（括号里的内容是和没训练过直接随机初始化的Bert表现的<strong>分差</strong>）</p>
<p>可以看到，Bert的各个层确实在由浅入深地学习语义信息。</p>
<h4 id="主谓一致"><a href="#主谓一致" class="headerlink" title="主谓一致"></a>主谓一致</h4><p>这个任务很有意思，个人理解是，在一句话中的主语和动词之间插入更多的名词进行噪声干扰，让模型预测动词的编号是多少。实验结果如下：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503210756744.png" alt="image-20220503210756744"></p>
<p>可以看出，对于中层的句法任务，插入的<strong>干扰</strong>越多，Bert越依赖<strong>更深层</strong>的网络来解决这个问题，也验证了Bert的网络越深或许在更加复杂的任务上会具有更强的表现。</p>
<h4 id="注意力机制学到了什么？"><a href="#注意力机制学到了什么？" class="headerlink" title="注意力机制学到了什么？"></a>注意力机制学到了什么？</h4><p>作者通过Tensor Product Decomposition Networks（TPDN）来研究注意力机制的结构，得到了下图的依赖树：</p>
<p><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/image-20220503211123026.png" alt="image-20220503211123026"></p>
<p>可以看出，注意力机制衍生的依赖树证明了Bert学习到了一些语法信息，这些依赖基本上与英语语法相吻合。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h4><p>个人认为Bert最大的贡献莫过于提供了一个可以被广泛应用的预训练模型，极大地推动了NLP领域的落地与应用。而且，Bert还可以迁移到多个语种上进行应用，不只局限于英语。</p>
<h4 id="局限"><a href="#局限" class="headerlink" title="局限"></a>局限</h4><p>前面埋了一个小彩蛋，说Bert被<strong>上了一把锁</strong>，那么具体是什么呢？其实，Bert的预训练策略导致它天然的不适合做自然语言生成（NLG）任务。因为NLG强调的是，我要根据当前的token和上文所有的一切来预测下一个token是什么，这是<strong>单向</strong>。而Bert的是双向的，它会自然地去结合上下文信息，这就导致它不适合NLG任务，或者机器翻译任务也不合适，因为它并没有使用transformer的<strong>解码器</strong>部分。像GPT采用单向的解码器，就可以适应NLG任务。</p>
<p>还有一点，是关于<strong>mask</strong>。预训练的时候，输入是有12%（15%*80%）带[mask]的。可是微调与inference的时候，输入是不带[mask]的，这会使得Bert不太适应，不知道怎么去处理，造成一些瓶颈。而且，WordPiece可能是对词根词缀做了mask，但是理论上应该要对整个词进行掩盖才对，这又衍生出了一个改进方向：Bert-WWM<strong>（whole-word-masking）</strong>。</p>
<p><strong>总之，Bert是里程碑式的工作，也是要理解当下众多自然语言处理模型的基础。所以做了比较详细的记录，特此感谢俊毅哥还有KNLP组中其他的小伙伴们！！</strong></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:1838040569@qq.com">Eric Miao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/05/01/BERT/">http://example.com/2022/05/01/BERT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Gimdong</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NLP/">NLP</a></div><div class="post_share"><div class="social-share" data-image="http://4myblog.oss-cn-beijing.aliyuncs.com/img/d1st99u-0493e064-ee96-4686-a7ef-e905aeecc12f.png" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/06/09/cow/"><img class="prev-cover" src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/os.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">写时复制技术与漏洞调研(COW and DIRTY COW)</div></div></a></div><div class="next-post pull-right"><a href="/2022/04/07/%E5%9C%A8%E9%87%91%E4%B8%AD%E7%9A%84%E6%97%A5%E5%8D%87%E6%97%A5%E8%90%BD%E3%80%82/"><img class="next-cover" src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/bg1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">持续更新--金中日子</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/03/24/CRF原理初探/" title="CRF原理初探"><img class="cover" src="https://4myblog.oss-cn-beijing.aliyuncs.com/img/wallhaven-p8581j_1920x1080.png?versionId=CAEQFhiBgMCOzMH7_RciIDVjNmE1NGY0YTAyZjQ4ZjFiOWE1MzNlZWI2MDZkYzEx" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-24</div><div class="title">CRF原理初探</div></div></a></div><div><a href="/2022/03/25/使用CRF进行分词任务/" title="使用CRF进行分词训练与推理"><img class="cover" src="https://4myblog.oss-cn-beijing.aliyuncs.com/img/wallhaven-4v9vx5_1600x1200.png?versionId=CAEQFhiBgMCnnMGG_hciIDE0NzQzMDNlYzI5YzQwOTZhYzgxNTQwY2E3MjdjODI5" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-25</div><div class="title">使用CRF进行分词训练与推理</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Twikoo</span><span class="switch-btn"></span><span class="second-comment">Gitalk</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Eric Miao</div><div class="author-info__description">我们到了一个不断说再见的时候</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ERICMIAO0817"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ERICMIAO0817" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1838040569@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">To think different.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Bert%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">1.</span> <span class="toc-text">Bert是什么？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bert%E8%A7%A3%E5%86%B3%E4%BA%86%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">Bert解决了什么问题？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B9%88%E9%87%8D%E8%A6%81%EF%BC%9F"><span class="toc-number">2.1.</span> <span class="toc-text">什么是预训练？为什么这么重要？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%82%A3NLP%E4%B8%BA%E4%BB%80%E4%B9%88%E5%88%B0Bert%E4%B9%8B%E5%89%8D%EF%BC%8C%E9%83%BD%E6%B2%A1%E6%9C%89%E8%BF%99%E6%A0%B7%E7%9A%84%E4%B8%80%E7%A7%8D%E4%BD%93%E7%B3%BB%EF%BC%9F"><span class="toc-number">2.2.</span> <span class="toc-text">那NLP为什么到Bert之前，都没有这样的一种体系？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8C%E5%90%91%E6%80%A7%EF%BC%88Bidirectional%EF%BC%89%E7%9A%84%E4%BD%93%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">双向性（Bidirectional）的体现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A9%E8%86%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E2%80%93MLM"><span class="toc-number">3.1.</span> <span class="toc-text">掩膜语言模型–MLM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%A5%E5%AD%90%E8%AF%AD%E5%BA%8F%E9%A2%84%E6%B5%8B%E2%80%93NSP"><span class="toc-number">3.2.</span> <span class="toc-text">句子语序预测–NSP</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer-Encoder"><span class="toc-number">4.</span> <span class="toc-text">Transformer Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFAttention%EF%BC%9F"><span class="toc-number">4.1.</span> <span class="toc-text">什么是Attention？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E2%80%9DMulti-Head%E2%80%9C%EF%BC%9F"><span class="toc-number">4.2.</span> <span class="toc-text">什么是”Multi-Head“？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bert%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84%E2%80%93%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">5.</span> <span class="toc-text">Bert基本结构–预训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Bert%E7%9A%84%E4%B8%89%E5%B1%82%E5%B5%8C%E5%85%A5"><span class="toc-number">5.1.</span> <span class="toc-text">Bert的三层嵌入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B4%E8%A7%82%E4%B8%80%E4%BA%9B%EF%BC%8C%E7%9C%8B%E7%9C%8B%E6%BA%90%E7%A0%81"><span class="toc-number">5.2.</span> <span class="toc-text">直观一些，看看源码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bert%E5%A6%82%E4%BD%95%E5%BE%AE%E8%B0%83"><span class="toc-number">6.</span> <span class="toc-text">Bert如何微调</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E6%8E%A8%E7%90%86%E2%80%93NLI"><span class="toc-number">6.1.</span> <span class="toc-text">自然语言推理–NLI</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E5%8F%A5%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E2%80%93%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E3%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90"><span class="toc-number">6.2.</span> <span class="toc-text">单句分类任务–文本分类、情感分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E4%BB%BB%E5%8A%A1%E2%80%93MRC"><span class="toc-number">6.3.</span> <span class="toc-text">阅读理解任务–MRC</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E9%97%AE%E9%A2%98%E2%80%93NER"><span class="toc-number">6.4.</span> <span class="toc-text">序列标注问题–NER</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%80%E4%BB%A5%EF%BC%8CBert%E5%88%B0%E5%BA%95%E5%AD%A6%E5%88%B0%E4%BA%86%E4%BB%80%E4%B9%88"><span class="toc-number">7.</span> <span class="toc-text">所以，Bert到底学到了什么</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%AD%E5%8F%A5%E5%8F%A5%E6%B3%95%E7%89%B9%E5%BE%81"><span class="toc-number">7.1.</span> <span class="toc-text">短句句法特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E7%BA%A7%E4%BB%BB%E5%8A%A1%E5%88%86%E6%9E%90"><span class="toc-number">7.2.</span> <span class="toc-text">三级任务分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BB%E8%B0%93%E4%B8%80%E8%87%B4"><span class="toc-number">7.3.</span> <span class="toc-text">主谓一致</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%AD%A6%E5%88%B0%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">7.4.</span> <span class="toc-text">注意力机制学到了什么？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">8.</span> <span class="toc-text">总结</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%A1%E7%8C%AE"><span class="toc-number">8.1.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B1%80%E9%99%90"><span class="toc-number">8.2.</span> <span class="toc-text">局限</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/10/18/Life-game/" title="康威的生命游戏"><img src="https://4myblog.oss-cn-beijing.aliyuncs.com/img/wallhaven-pkle1j_1920x1200.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="康威的生命游戏"/></a><div class="content"><a class="title" href="/2022/10/18/Life-game/" title="康威的生命游戏">康威的生命游戏</a><time datetime="2022-10-18T08:39:02.000Z" title="发表于 2022-10-18 16:39:02">2022-10-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/10/self-%20management/" title="管好你自己"><img src="https://4myblog.oss-cn-beijing.aliyuncs.com/img/wallhaven-k9o36m_1920x1080.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="管好你自己"/></a><div class="content"><a class="title" href="/2022/10/10/self-%20management/" title="管好你自己">管好你自己</a><time datetime="2022-10-10T08:39:02.000Z" title="发表于 2022-10-10 16:39:02">2022-10-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/08/17/birthday/" title="无题"><img src="/img/gimdong.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2022/08/17/birthday/" title="无题">无题</a><time datetime="2022-08-16T16:17:37.915Z" title="发表于 2022-08-17 00:17:37">2022-08-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/18/2022-07-18-Macbook%20Air/" title="First try on Mac"><img src="https://4myblog.oss-cn-beijing.aliyuncs.com/img/wallhaven-e7mjql_2880x1800+11.18.03.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="First try on Mac"/></a><div class="content"><a class="title" href="/2022/07/18/2022-07-18-Macbook%20Air/" title="First try on Mac">First try on Mac</a><time datetime="2022-07-18T13:39:02.000Z" title="发表于 2022-07-18 21:39:02">2022-07-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/09/cow/" title="写时复制技术与漏洞调研(COW and DIRTY COW)"><img src="http://4myblog.oss-cn-beijing.aliyuncs.com/img/os.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="写时复制技术与漏洞调研(COW and DIRTY COW)"/></a><div class="content"><a class="title" href="/2022/06/09/cow/" title="写时复制技术与漏洞调研(COW and DIRTY COW)">写时复制技术与漏洞调研(COW and DIRTY COW)</a><time datetime="2022-06-09T13:39:02.000Z" title="发表于 2022-06-09 21:39:02">2022-06-09</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Eric Miao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://ericcsu.top/',
      region: ''
    }, null))
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'https://ericcsu.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '5057c18799d5f3ee0784',
      clientSecret: 'f462783e432c8e05420145bf94a09eb2688a61fd',
      repo: 'ERICMIAO0817.github.io',
      owner: 'ERICMIAO0817',
      admin: ['ERICMIAO0817'],
      id: '3617335eeda3f601f08488d5de07c9f8',
      language: 'en',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: false,
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Twikoo' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>